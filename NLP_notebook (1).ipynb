{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Understanding NLP & Sentiment Analysis in Yelp Review Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Natural Language Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Is a field that is at the intersection of Computer science, Computational lingustics and Artificial Intelligence that allows a computer program to understand human speech as it is spoken.\n",
    "- focuses on the interactions between human language and computers.\n",
    "- Allows machine to understand how human speak.\n",
    "- Is used for sentiment analysis, topic extraction,speech tagging, relationship extraction, stemming\n",
    "- Is a very hard filed as human's speeches are not precisely stated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing and Understanding Text\n",
    "\n",
    "- Since texts come as one of the most structureless data, it is more than crucial to make it usable. This is commonly known as pre-processing it.\n",
    "- To palliate to that one can must:\n",
    "    * Remove noise from the text.\n",
    "    * Normalize the text.(stemming, lemmatizing, changing characters to \n",
    "    lower cases,expanding abbreviations, removing stopwords\n",
    "    * Standarize the text. (In general, via the use of regular expression)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above cited approaches fall into the text pre-processing stage. At this stage, one should remember that the text should be transformed into features so that the machine learning algorithms can really digest it. For, the commonly used techniques are:\n",
    "    * N-grams - constits at forming combinations of N words\n",
    "    * Statistical features( Terme frequency and inverse document frequence\n",
    "    * Syntactic Parsing - Syntactic parsing, then, is the task of recognizing sentence and \n",
    "    assigning a syntactic structure to it.\n",
    "    * Entity extraction - Defining or making the most important parts of a sentence. \n",
    "    * Word Embedding - aims at transforming high dimensional words as lower dimensional \n",
    "    vectors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NLP is mainly based on:\n",
    "- Tokens are words present in a text\n",
    "- Tokenization which can be defined as the process of converting a text into tokens\n",
    "- Text Sentence can be looked at as the paragraphs, sentences and words that are present in a given text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Words!\n",
    "- In a natural language processing, words come into two forms:\n",
    "    - inflections :\n",
    "        - Adding a suffix to the word does not change its grammatical category. \n",
    "            - Plural with nouns\n",
    "    - derivations :\n",
    "        - Adding a suffix to the word does change its grammatical category.\n",
    "            - beauty ---- beautiful, nation--- national"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#sentence Tokenize: sent_tokenize()\n",
    "#word tokenize:\n",
    "#Part of Speech tagging: pos_tag()\n",
    "#stemming(word root) nltk.stem import SnowballStemmer stem() - finds linguistic basis of word\n",
    "#lemmatization: nltk.stem import WordNetLemmatizer - finds conceptual basis of word\n",
    "#Named Entity Recognizer: nltk.tag.stanford import NERTagger - classify text elements into pre-defined categories\n",
    "#spelling correction correct()\n",
    "#translation and language detection:  from langdetect import detect - detect()\n",
    "#Text Blob .detect_language() .translate()\n",
    "#TF-IDF is term frequency inverse document frequency: term freq = # of times word in document, doc freq = # of docs word is in\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied (use --upgrade to upgrade): nltk in c:\\users\\sdanioko\\anaconda3\\lib\\site-packages\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.util import ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello', 'world']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "word_tokenize(\"Hello world\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'am', 'aware', 'that', 'nltk', 'only', 'offers', 'bigrams', 'and', 'trigrams', ',', 'but', 'is', 'there', 'a', 'way', 'to', 'split', 'my', 'text', 'in', 'four-grams', ',', 'five-grams', 'or', 'even', 'hundred-grams']\n"
     ]
    }
   ],
   "source": [
    "text = \"I am aware that nltk only offers bigrams and trigrams, but is there a way to split my text in four-grams, five-grams or even hundred-grams\"\n",
    "tokenize = nltk.word_tokenize(text)\n",
    "print(tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['PierreVinke', ',', '59', 'years', 'old', ',', 'will', 'join', 'as', 'a', 'nonexecutive', 'director', 'on', 'Nov', '.', '29', '.']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "# word_tokenize is used to find the list of words in strings\n",
    "text = nltk.word_tokenize(\"PierreVinke, 59 years old, will join as a nonexecutive director on Nov . 29.\")\n",
    "print(text) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If there are any missing nltk modules, (nlt.xxxx), call nltk.download()\n",
    "\n",
    "You can either download the missing modules individually, or download all packages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is the difference between split and world_tokenize?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I', 'am', 'Sidy', 'danioko,', 'from', 'west', 'africa']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "message = 'I am Sidy danioko, from west africa'\n",
    "message.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I', 'am', 'Sidy', 'danioko', ',', 'from', 'west', 'africa']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_tokenize(message) # word tokenize tokenisizes by word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['This', \"'s\", 'a', 'car']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_tokenize(\"This's a car\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentence Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Welcome readers.', 'I hope you find it interesting.', 'Please do reply']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "# sent_tokenize tokenizes by sentences. It is used to find the list of sentences \n",
    "text=\"Welcome readers. I hope you find it interesting. Please do reply\"\n",
    "print(sent_tokenize(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TreeBankWordTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Treebank tokenizer uses regular expressions to tokenize texts\n",
    "# Assums that the text has already been segmented into sentences via the use of sent_tokenize()\n",
    "# split standard contractions exp she's - 'she', 's', she'll- 'she','ll'\n",
    "# looks at puctuactions as separate tokens\n",
    "# splits off commas and single quotes, whenver they are followed by whitespace\n",
    "# separate periods that appear at the end of line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Have', 'a', 'nice', 'day.', 'I', 'hope', 'you', 'find', 'the', 'book', 'interesting', '.']\n",
      "['Do', \"n't\", 'hesitate', 'to', 'ask', 'questions']\n",
      "['I', 'want', 'to', 'by', 'this', 'pen', 'for', '$', '44.15']\n",
      "['Hello', ',', 'I', 'am', 'sidy']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "# Treebank tokenizer uses regualar expressions to tokenize text\n",
    "tokenizer = TreebankWordTokenizer()\n",
    "print (tokenizer.tokenize(\"Have a nice day. I hope you find the book interesting.\"))\n",
    "print (tokenizer.tokenize(\"Don't hesitate to ask questions\"))\n",
    "print (tokenizer.tokenize(\" I want to by this pen for $44.15\"))\n",
    "print (tokenizer.tokenize(\" Hello,I am sidy\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### WordPunctTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This tokenizes a text into a sequence of alphabetic and non-alphabetic\n",
    "# characters, using regular exopression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Don', \"'\", 't', 'hesitate', 'to', 'ask', 'questions']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import WordPunctTokenizer\n",
    "tokenizer = WordPunctTokenizer()\n",
    "print (tokenizer.tokenize(\"Don't hesitate to ask questions\"))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# use wordPunctTokenizer to tokenize\n",
    "str = 'I am selling my cat for $2,220.34. Do you have money to\n",
    "buy\\nthree cats.\\n\\n does'snt she'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RegexpTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This split a string into substrings using a regualr expression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['She', 'She']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "sent = \"She secures 90.56% in class X.  She is a meritorious student. Go \"\n",
    "capt = RegexpTokenizer('[A-Z]\\w+')\n",
    "capt.tokenize(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# write a function that takes a text and give the index of similar consecutive word\n",
    "def adj_location(text,target):\n",
    "    splitted = text.split()\n",
    "    for n in range(len(splitted)-1):\n",
    "        if splitted[n]== target and splitted[n+1]== target:\n",
    "            print (n,n+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 3\n",
      "3 4\n",
      "4 5\n"
     ]
    }
   ],
   "source": [
    "adj_location('I am sidy sidy sidy sidy is nice','sidy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def adj_location(text):\n",
    "    splitted = text.split()\n",
    "    for i in range(len(splitted)-1):\n",
    "        if splitted[i]== splitted[i+1]:\n",
    "            print(splitted[i],i,i+1)\n",
    "            print(i,i+1,splitted[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def count(text):\n",
    "    splitted = text.split()\n",
    "    d = {}\n",
    "    for elem in splitted:\n",
    "        if elem in d:\n",
    "            d[elem]+=1\n",
    "        else:\n",
    "            d[elem]=1\n",
    "    return(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "text = ' I am a Ndeye from senegal. I am not crazy'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'I': 2,\n",
       " 'Ndeye': 1,\n",
       " 'a': 1,\n",
       " 'am': 2,\n",
       " 'crazy': 1,\n",
       " 'from': 1,\n",
       " 'not': 1,\n",
       " 'senegal.': 1}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = \"\"\" Google Analytics is very very very nice (now)\n",
    "By Jason Hoffman 18 August 06\n",
    "Google Analytics, the result of Google’s acquisition of the San\n",
    "Diego-based Urchin Software Corporation, really really opened its\n",
    "doors to the world a couple of days ago, and it allows you to\n",
    "track up to 10 sites within a single google account.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 4\n",
      "4 5\n"
     ]
    }
   ],
   "source": [
    "adj_location(s,'very')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "adj_location(s,'o')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Google Analytics is {very very} very nice (now)\n",
      "By Jason Hoffman 18 August 06\n",
      "Google Analytics, the result of Google’s acquisition of the San\n",
      "Diego-based Urchin Software Corporation, really really opened its\n",
      "doors to the world a couple of days ago, and it allows you to\n",
      "track up to 10 sites within a single google account.\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.re_show('very very',s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "G{oo}gle Analytics is very very very nice (n{o}w)\n",
      "By Jas{o}n H{o}ffman 18 August 06\n",
      "G{oo}gle Analytics, the result {o}f G{oo}gle’s acquisiti{o}n {o}f the San\n",
      "Dieg{o}-based Urchin S{o}ftware C{o}rp{o}rati{o}n, really really {o}pened its\n",
      "d{oo}rs t{o} the w{o}rld a c{o}uple {o}f days ag{o}, and it all{o}ws y{o}u t{o}\n",
      "track up t{o} 10 sites within a single g{oo}gle acc{o}unt.\n"
     ]
    }
   ],
   "source": [
    "nltk.re_show('o+',s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "G{oo}gle Analytics is very very very nice (now)\n",
      "By Jason Hoffman 18 August 06\n",
      "G{oo}gle Analytics, the result of G{oo}gle’s acquisition of the San\n",
      "Diego-based Urchin Software Corporation, really really opened its\n",
      "d{oo}rs to the world a couple of days ago, and it allows you to\n",
      "track up to 10 sites within a single g{oo}gle account.\n"
     ]
    }
   ],
   "source": [
    "nltk.re_show('oo+',s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['She secures', '90.56% in class X.', 'She is a meritorious student']"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import BlanklineTokenizer\n",
    "sent = '''She secures\n",
    "\n",
    "90.56% in class X.  \n",
    "\n",
    "She is a meritorious student'''\n",
    "BlanklineTokenizer().tokenize(sent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stemmer & Lemmatizer\n",
    "- Stemmer and lemmatizer are two methods to handle inflections. \n",
    "- Stemming and lemmatization tend to \"normalize\" words to their common base form,\n",
    "- Stemmers aim to remove the morphological affixes from words, leaving only the world stem.\n",
    "- Lemmatisation is to bring a word to its conventional form as it is a dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PorterStemming & PorterStemmer\n",
    "- Porter stemming is an algorithm, a collection of rules that provides ways to better handle English inflections.\n",
    "- It is a process of removing suffixes from words in english.\n",
    "- Very important in information retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "stemmerporter = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "talk\n",
      "happi\n",
      "happi\n",
      "unhappi\n",
      "ran\n",
      "is\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "stemmerporter = PorterStemmer()\n",
    "print(stemmerporter.stem('talking'))\n",
    "print (stemmerporter.stem('happiness'))\n",
    "print (stemmerporter.stem('happy'))\n",
    "print (stemmerporter.stem('unhappy'))\n",
    "print (stemmerporter.stem('ran'))\n",
    "print (stemmerporter.stem('is'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 4, 9, 16]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inT = [1,2,3,4]\n",
    "list(map(lambda x: x**2,inT))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 4, 9, 16]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g = lambda x: x**2\n",
    "[g(elem) for elem in inT]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "words = ['houses', 'trains', 'pens', 'cars', 'eaten','sick', \n",
    "         'nice', 'bought', 'selling', 'sized',\n",
    "           'speech', 'rolling', 'marching', 'identification',\n",
    "         'universal', 'beautiful', 'references', 'countries',\n",
    "         'called']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "[burger for burgen in words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hous', 'train', 'pen', 'car', 'eaten', 'sick', 'nice', 'bought', 'sell', 'size', 'speech', 'roll', 'march', 'identif', 'univers', 'beauti', 'refer', 'countri', 'call']\n"
     ]
    }
   ],
   "source": [
    "single = [stemmerporter.stem(elem) for elem in words]\n",
    "print(single)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 4, 9, 16, 25, 36, 49]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f = lambda x: x**2\n",
    "list_ints = [1,2,3,4,5,6,7]\n",
    "list(map(f,list_ints))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 4, 9, 16, 25, 36, 49]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[f(elem) for elem in list_ints]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "{word: stemmerpoter.stem(word) for word in words}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LancasterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "happy\n",
      "happy\n",
      "unhappy\n",
      "ran\n",
      "is\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.stem import LancasterStemmer\n",
    "stemmerLan = LancasterStemmer()\n",
    "print (stemmerLan.stem('happiness'))\n",
    "print (stemmerLan.stem('happy'))\n",
    "print (stemmerLan.stem('unhappy'))\n",
    "print (stemmerLan.stem('ran'))\n",
    "print (stemmerLan.stem('is'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RegexpStemmer\n",
    "- Regex is the short of regular expression.\n",
    "- What is a regular expression?\n",
    "    - It is a special text string that aims to describe a search pattern\n",
    "-  Uses regular expressions to identify morphological affixes. As such, a given substring that matches the regular expressions will be automatically removed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "work\n",
      "happiness\n",
      "pair\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.stem import RegexpStemmer\n",
    "stemmerreg = RegexpStemmer('ing')\n",
    "print (stemmerreg.stem('working'))\n",
    "print (stemmerreg.stem('happiness'))\n",
    "print (stemmerreg.stem('pairing'))\n",
    "print(stemmerreg.stem('singde'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SnowballStemmer\n",
    "- It contains 16 stemmer algorithms (Danish,Dutch, English, Finnish, French, German, Hungarian,...)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('danish', 'dutch', 'english', 'finnish', 'french', 'german', 'hungarian', 'italian', 'norwegian', 'porter', 'portuguese', 'romanian', 'russian', 'spanish', 'swedish')\n",
      "com\n",
      "mang\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.stem import SnowballStemmer\n",
    "print (SnowballStemmer.languages)\n",
    "spanishstemmer = SnowballStemmer('spanish')\n",
    "print (spanishstemmer.stem('comiendo'))\n",
    "\n",
    "frenchstemmer = SnowballStemmer('french')\n",
    "print (frenchstemmer.stem('manger'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dans\n"
     ]
    }
   ],
   "source": [
    "frechstemmer = SnowballStemmer('french')\n",
    "print (frenchstemmer.stem('danser'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemmatization\n",
    "- Stands for doing things in the right way based on the use of a vocabulary and that of the morphological analysis of words\n",
    "- Aims at removing inflectional endings only. Its purposes is to return the base or the directional form of a given word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "work\n",
      "run\n",
      "take\n",
      "be\n",
      "happiness\n",
      "took\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer_output = WordNetLemmatizer()\n",
    "print (lemmatizer_output.lemmatize('working', pos='v'))\n",
    "print (lemmatizer_output.lemmatize('ran', pos='v'))\n",
    "print (lemmatizer_output.lemmatize('took', pos='v'))\n",
    "print (lemmatizer_output.lemmatize('is', pos='v'))\n",
    "print (lemmatizer_output.lemmatize('happiness'))\n",
    "print (lemmatizer_output.lemmatize('took'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part of speech tagging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Tags available at Penn Treebank\n",
    "- Tagging, also known as grammatical tagging is a process of classifying words in their part of speech and label then accordingly\n",
    "- exple : \n",
    "        - conjoction of coordinations get mapped to cc\n",
    "        - adverbs get mapped to RB\n",
    "        - prepositions get mapped to IN\n",
    "        - something gets mapped to NN\n",
    "        - adjectives get mapped to jj\n",
    "        - verbs get mapped to VBZ\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 4, 6, 8]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ls = [1,2,3,4,5,6,8]\n",
    "list(filter(lambda x: x%2==0,ls))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "list(filter(lambda x: len(x) > 4, words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "list(filter(lambda sidy: sidy[-1]=='s',words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('It', 'PRP'),\n",
       " ('is', 'VBZ'),\n",
       " ('a', 'DT'),\n",
       " ('pleasant', 'JJ'),\n",
       " ('day', 'NN'),\n",
       " ('today', 'NN')]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "text = word_tokenize(\"It is a pleasant day today\")\n",
    "nltk.pos_tag(text) #pos_tagger stands for part of speech tagger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "list_of_words = nltk.pos_tag(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('It', 'PRP'),\n",
       " ('is', 'VBZ'),\n",
       " ('a', 'DT'),\n",
       " ('pleasant', 'JJ'),\n",
       " ('day', 'NN'),\n",
       " ('today', 'NN')]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_of_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('day', 'NN'), ('today', 'NN')]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(filter(lambda x: x[1]=='NN',list_of_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('They', 'PRP'),\n",
       " ('buy', 'VBP'),\n",
       " ('the', 'DT'),\n",
       " ('permit', 'NN'),\n",
       " ('in', 'IN'),\n",
       " ('order', 'NN'),\n",
       " ('to', 'TO'),\n",
       " ('be', 'VB'),\n",
       " ('able', 'JJ'),\n",
       " ('to', 'TO'),\n",
       " ('attend', 'VB'),\n",
       " ('the', 'DT'),\n",
       " ('event', 'NN')]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = word_tokenize(\"They buy the permit in order to be able to attend the event\")\n",
    "nltk.pos_tag(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tag(text,model):\n",
    "    splitted = text.split()\n",
    "    tagged = nltk.pos_tag(splitted)\n",
    "    selected = list(filter(lambda w:w[-1]== model,tagged))\n",
    "    return(selected)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tag(t'Sidy is here','NN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Beautiful', 'He is the man'), ('morning', 'He is the man')]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tag import DefaultTagger\n",
    "tag = DefaultTagger('He is the man')\n",
    "tag.tag(['Beautiful', 'morning'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Language conversion & Text formatting & Grammar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "!pip install --trusted-host pypi.python.org autocorrect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from autocorrect import spell\n",
    "spell(\"Tghe\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TextBlob\n",
    "- It is a python library for textual data processing. It provides an interface for common natural language processing tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --trusted-host pypi.python.org textblob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nl\n",
      "I have good spelling!\n"
     ]
    }
   ],
   "source": [
    "from textblob import TextBlob\n",
    "b = TextBlob(\"I havv good speling!\")\n",
    "print(b.detect_language())\n",
    "print (b.correct())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('fallibility', 0.3333333333333333),\n",
       " ('capability', 0.3333333333333333),\n",
       " ('affability', 0.3333333333333333)]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from textblob import Word\n",
    "w = Word('falability')\n",
    "w.spellcheck()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['tip laterally',\n",
       " 'enclose with a bank',\n",
       " 'do business with a bank or keep an account at a bank',\n",
       " 'act as the banker in a game or in gambling',\n",
       " 'be in the banking business',\n",
       " 'put into a bank account',\n",
       " 'cover with ashes so to control the rate of burning',\n",
       " 'have confidence or faith in']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# definitions\n",
    "Word('bank').define('v')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'es'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# language identification\n",
    "TextBlob('Hola amigos').detect_language()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --trusted-host pypi.python.org langdetect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "en\n",
      "de\n",
      "pt\n"
     ]
    }
   ],
   "source": [
    "from langdetect import detect\n",
    "print (detect(\"War doesn't show who's right, just who's left.\"))\n",
    "print (detect(\"Ein, zwei, drei, vier\"))\n",
    "print (detect(\"Eu gosto de mulher\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TextBlob(\"Eu sou um homem negro livre amado por Jesus Cristo.\")"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#en_blob = TextBlob(u'Simple is better than complex.')\n",
    "#en_blob.translate(to='vi') # vi stands for vietnamese\n",
    "en_blob = TextBlob(u'I am a free black man loved by Jesus Christ.')\n",
    "en_blob.translate(to='pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from textblob import Word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "blob = TextBlob(\"ITP is a two-year graduate program located in the Tisch School of the Arts. Perhaps the best way to describe us is as a Center for the Recently Possible.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Sentence(\"ITP is a two-year graduate program located in the Tisch School of the Arts.\"),\n",
       " Sentence(\"Perhaps the best way to describe us is as a Center for the Recently Possible.\")]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "blob.sentences\n",
    "blob.words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "75\n",
      "77\n"
     ]
    }
   ],
   "source": [
    "for sentence in blob.sentences:\n",
    "    print (len(sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perhaps the best way to describe us is as a Center for the Recently Possible.\n"
     ]
    }
   ],
   "source": [
    "print (blob.sentences[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Perhaps', 'the', 'best', 'way', 'to', 'describe', 'us', 'is', 'as', 'a', 'Center', 'for', 'the', 'Recently', 'Possible']\n"
     ]
    }
   ],
   "source": [
    "print (blob.sentences[1].words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n",
      "3\n",
      "4\n",
      "3\n",
      "2\n",
      "8\n",
      "2\n",
      "2\n",
      "2\n",
      "1\n",
      "6\n",
      "3\n",
      "3\n",
      "8\n",
      "8\n"
     ]
    }
   ],
   "source": [
    "for word in blob.sentences[1].words:\n",
    "    print (len(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WordList(['ITP', 'is', 'a', 'two-year', 'graduate', 'program', 'located', 'in', 'the', 'Tisch', 'School', 'of', 'the', 'Arts', 'Perhaps', 'the', 'best', 'way', 'to', 'describe', 'us', 'is', 'as', 'a', 'Center', 'for', 'the', 'Recently', 'Possible'])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "blob.words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cars\n"
     ]
    }
   ],
   "source": [
    "from textblob import Word\n",
    "word = Word('Car')\n",
    "print word.pluralize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sentiment(polarity=0.3066666666666667, subjectivity=0.7166666666666667)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TextBlob('Sidy is a very nice and inteligent black man').sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sentiment(polarity=0.85, subjectivity=1.0)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TextBlob('Beautiful').sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sentiment(polarity=-0.425, subjectivity=1.0)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TextBlob('not beautiful').sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentiment(polarity=-0.35, subjectivity=0.6000000000000001)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Sentiment(polarity=-0.35, subjectivity=0.6000000000000001)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Textblob does not consider words that are made of one letter in the \n",
    "# sentiment analysis\n",
    "print TextBlob('not a good person').sentiment\n",
    "TextBlob('not good person').sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# One can use to get the frequency of a word or noun phrase via TextBlob\n",
    "message = TextBlob('Mali is located in west africa.',\n",
    "                  'Back in the days, Mali is used to be a french colony. '\n",
    "                  'Now, despite being so called an independent country,\\n\n",
    "                   Mali still depends on france')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "message.word_counts['Mali'] # In this way, the search will not be case sensitive. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "message.words.count('Mali')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# To overcome the case sensitivity, one can specify\n",
    "message.words.count('Mali', case_sensitive = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WordList(['Mali', 'is', 'located', 'in', 'west', 'africa.Back', 'in', 'the', 'days', 'Mali', 'is', 'used', 'to', 'be', 'a', 'french', 'colony', 'Now', 'despite', 'being', 'so', 'called', 'an', 'independent', 'country', 'Mali', 'still', 'depends', 'on', 'france'])"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "message.words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'day'"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "message.words[8].singularize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'colonies'"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "message.words[16].pluralize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF    Term Frequency - Inverse Document Frequency \n",
    "- It is a statistical tool that aims to reflect how much important a word is to a document in a collection or corpus.\n",
    "- It can be seen as a weighting factor.\n",
    "- How to generate TF-IDF of phrases of Tokens?\n",
    "   - by using CountVectorizer(what the hell is this?) then feeding the output of that into TfidfTransformer.\n",
    "   - by directly inputing the collection of text or documents to TfidfVectorizer "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is CountVectorizer?\n",
    "- It is a technique that allows to:\n",
    "    * tokenize a collection of text documents.\n",
    "    * build a vocabulary of known words\n",
    "    * encode new documents using that vocabulary\n",
    "\n",
    "- To use CountVectorizer one should:\n",
    "    * Make an instance(in terms of class) of CountVectorizer\n",
    "    * Use fit() function so that they can be able to learn a vocabulary from different documents\n",
    "    * Use transform() function on the available documents, which facilitates the encoding process\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Frequencies with TfidfVectorizer\n",
    "- In the above case, we use the basic word counts that has its own limits.\n",
    "- Therefore, we need a more robust method to calculate the word frequencies. And, by far, the most commonly used method is TF-IDF\n",
    "    * TF, which is the short hand for Term Frequency, tells us how often a given word appears within a given document\n",
    "    * IDF, short hand for Inverse Document Frequency, aims at bringing words that appear alot to the the lower end of the scale \n",
    "    \n",
    "- TF-IDF can be seen as a word frequency scoring technique that searches for words that have more weights. In terms of frequency, we can say words that appear alot in a given ocument but not across documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extraction of numerical features from texts\n",
    "- Tokenize strings and  attach an integer, called ID, to each obtained token\n",
    "- Count the number of times each token appears in a given document\n",
    "- Normalize over the occurences in the majority of the documents\n",
    "- In doing so, we should note that the frequency of each token is a FEATURE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Corpus\n",
    "- a collection of writen texts or documents"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "A corpus of documents can always be represented as a matrix where one row represent a specific document and each column denotes the occuring of a token (e.g. word) in a given corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "To vectorize (vectorization) aims at turning a collection of text documents into numerical feature vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Questions?\n",
    "# what is the difference between fit(), \n",
    "#fit() : is used to generate learning model parameters from training data\n",
    "#transform() : parameters generated from fit() method,applied upon model to\n",
    "# generate transformed data set.\n",
    "# fit_transform() : combines fit() and transform() applied on same data sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# - fit Learn vocabulary and idf from training set.\n",
    "# fit_transform Learn vocabulary and idf, return term-document matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cab</th>\n",
       "      <th>call</th>\n",
       "      <th>me</th>\n",
       "      <th>please</th>\n",
       "      <th>tonight</th>\n",
       "      <th>you</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   cab  call  me  please  tonight  you\n",
       "0    0     1   0       0        1    1\n",
       "1    1     1   1       0        0    0\n",
       "2    0     1   1       2        0    0"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import scipy as sp\n",
    "import pandas as pd\n",
    "# we need to import and instantiate CountVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "simple_train = ['Call you tonight', 'Call me a cab', 'please call me... PLEASE!']\n",
    "\n",
    "vect=CountVectorizer() # CountVectorizer allows one Convert a collection of text\n",
    "# documents to a matrix of token count. The outputed matrix is a sparse one. What is\n",
    "# a sparse matrix?\n",
    "tf = pd.DataFrame(vect.fit_transform(simple_train).toarray(), columns=vect.get_feature_names())\n",
    "# Take the text or the document and learn the vocabulary\n",
    "tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# we can see that it is not displaying the character a. This is mainly because the default\n",
    "# Check on this case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cab</th>\n",
       "      <th>call</th>\n",
       "      <th>me</th>\n",
       "      <th>please</th>\n",
       "      <th>tonight</th>\n",
       "      <th>you</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   cab  call  me  please  tonight  you\n",
       "0    1     3   2       1        1    1"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vect = CountVectorizer(binary=True)\n",
    "df = vect.fit_transform(simple_train).toarray().sum(axis=0) # why does axis have to be zero?\n",
    "pd.DataFrame(df.reshape(1,6), columns=vect.get_feature_names())\n",
    "# This is about the document frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cab</th>\n",
       "      <th>call</th>\n",
       "      <th>me</th>\n",
       "      <th>please</th>\n",
       "      <th>tonight</th>\n",
       "      <th>you</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.5</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   cab      call   me  please  tonight  you\n",
       "0  0.0  0.333333  0.0     0.0      1.0  1.0\n",
       "1  1.0  0.333333  0.5     0.0      0.0  0.0\n",
       "2  0.0  0.333333  0.5     2.0      0.0  0.0"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf/df # why is that? What is the purpose?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cab</th>\n",
       "      <th>call</th>\n",
       "      <th>me</th>\n",
       "      <th>please</th>\n",
       "      <th>tonight</th>\n",
       "      <th>you</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.385372</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.652491</td>\n",
       "      <td>0.652491</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.720333</td>\n",
       "      <td>0.425441</td>\n",
       "      <td>0.547832</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.266075</td>\n",
       "      <td>0.342620</td>\n",
       "      <td>0.901008</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        cab      call        me    please   tonight       you\n",
       "0  0.000000  0.385372  0.000000  0.000000  0.652491  0.652491\n",
       "1  0.720333  0.425441  0.547832  0.000000  0.000000  0.000000\n",
       "2  0.000000  0.266075  0.342620  0.901008  0.000000  0.000000"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vect = TfidfVectorizer()\n",
    "pd.DataFrame(vect.fit_transform(simple_train).toarray(), columns=vect.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CountVectorizer - Fit Transform with NLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CountVectorizer(analyzer=u'word', binary=False, decode_error=u'strict',\n",
      "        dtype=<type 'numpy.int64'>, encoding=u'utf-8', input=u'content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern=u'(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>and</th>\n",
       "      <th>document</th>\n",
       "      <th>first</th>\n",
       "      <th>is</th>\n",
       "      <th>one</th>\n",
       "      <th>second</th>\n",
       "      <th>the</th>\n",
       "      <th>third</th>\n",
       "      <th>this</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   and  document  first  is  one  second  the  third  this\n",
       "0    0         1      1   1    0       0    1      0     1\n",
       "1    0         1      0   1    0       2    1      0     1\n",
       "2    1         0      0   0    1       0    1      1     0\n",
       "3    0         1      1   1    0       0    1      0     1"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer  = CountVectorizer(min_df=1) # min_df represents a threshold. Here, it puts a\n",
    "# constraint by telling while building the vocabulary you need to ignore terms \n",
    "# that have a document frequency strictly lower than the given threshold\n",
    "print (vectorizer)\n",
    "\n",
    "corpus = ['This is the first document','This is the second second document',\n",
    "          'And the third one', 'Is this the first document?']\n",
    "\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "\n",
    "tf = pd.DataFrame(X.toarray(), columns=vectorizer.get_feature_names())\n",
    "tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 8)\t1\n",
      "  (0, 3)\t1\n",
      "  (0, 6)\t1\n",
      "  (0, 2)\t1\n",
      "  (0, 1)\t1\n",
      "  (1, 8)\t1\n",
      "  (1, 3)\t1\n",
      "  (1, 6)\t1\n",
      "  (1, 1)\t1\n",
      "  (1, 5)\t2\n",
      "  (2, 6)\t1\n",
      "  (2, 0)\t1\n",
      "  (2, 7)\t1\n",
      "  (2, 4)\t1\n",
      "  (3, 8)\t1\n",
      "  (3, 3)\t1\n",
      "  (3, 6)\t1\n",
      "  (3, 2)\t1\n",
      "  (3, 1)\t1\n"
     ]
    }
   ],
   "source": [
    "print (X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.util import ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "message = 'nltk offers many types of ngrams. Among many others, the ones that \\\n",
    "are used the most are bigrams and trigrams. However, one can go ten grams,\\\n",
    "hundred and even thousand grams depending on what they are trying to go'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'nltk offers many types of ngrams. Among many others, the ones that are used the most are bigrams and trigrams. However, one can go ten grams,hundred and even thousand grams depending on what they are trying to go'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Let's tokenize this\n",
    "my_tokens = nltk.word_tokenize(message)\n",
    "#tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('nltk', 'offers'),\n",
       " ('offers', 'many'),\n",
       " ('many', 'types'),\n",
       " ('types', 'of'),\n",
       " ('of', 'ngrams'),\n",
       " ('ngrams', '.'),\n",
       " ('.', 'Among'),\n",
       " ('Among', 'many'),\n",
       " ('many', 'others'),\n",
       " ('others', ','),\n",
       " (',', 'the'),\n",
       " ('the', 'ones'),\n",
       " ('ones', 'that'),\n",
       " ('that', 'are'),\n",
       " ('are', 'used'),\n",
       " ('used', 'the'),\n",
       " ('the', 'most'),\n",
       " ('most', 'are'),\n",
       " ('are', 'bigrams'),\n",
       " ('bigrams', 'and'),\n",
       " ('and', 'trigrams'),\n",
       " ('trigrams', '.'),\n",
       " ('.', 'However'),\n",
       " ('However', ','),\n",
       " (',', 'one'),\n",
       " ('one', 'can'),\n",
       " ('can', 'go'),\n",
       " ('go', 'ten'),\n",
       " ('ten', 'grams'),\n",
       " ('grams', ','),\n",
       " (',', 'hundred'),\n",
       " ('hundred', 'and'),\n",
       " ('and', 'even'),\n",
       " ('even', 'thousand'),\n",
       " ('thousand', 'grams'),\n",
       " ('grams', 'depending'),\n",
       " ('depending', 'on'),\n",
       " ('on', 'what'),\n",
       " ('what', 'they'),\n",
       " ('they', 'are'),\n",
       " ('are', 'trying'),\n",
       " ('trying', 'to'),\n",
       " ('to', 'go')]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's create my bigrams\n",
    "bigrams = ngrams(my_tokens,2)\n",
    "list(bigrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('nltk', 'offers', 'many'),\n",
       " ('offers', 'many', 'types'),\n",
       " ('many', 'types', 'of'),\n",
       " ('types', 'of', 'ngrams'),\n",
       " ('of', 'ngrams', '.'),\n",
       " ('ngrams', '.', 'Among'),\n",
       " ('.', 'Among', 'many'),\n",
       " ('Among', 'many', 'others'),\n",
       " ('many', 'others', ','),\n",
       " ('others', ',', 'the'),\n",
       " (',', 'the', 'ones'),\n",
       " ('the', 'ones', 'that'),\n",
       " ('ones', 'that', 'are'),\n",
       " ('that', 'are', 'used'),\n",
       " ('are', 'used', 'the'),\n",
       " ('used', 'the', 'most'),\n",
       " ('the', 'most', 'are'),\n",
       " ('most', 'are', 'bigrams'),\n",
       " ('are', 'bigrams', 'and'),\n",
       " ('bigrams', 'and', 'trigrams'),\n",
       " ('and', 'trigrams', '.'),\n",
       " ('trigrams', '.', 'However'),\n",
       " ('.', 'However', ','),\n",
       " ('However', ',', 'one'),\n",
       " (',', 'one', 'can'),\n",
       " ('one', 'can', 'go'),\n",
       " ('can', 'go', 'ten'),\n",
       " ('go', 'ten', 'grams'),\n",
       " ('ten', 'grams', ','),\n",
       " ('grams', ',', 'hundred'),\n",
       " (',', 'hundred', 'and'),\n",
       " ('hundred', 'and', 'even'),\n",
       " ('and', 'even', 'thousand'),\n",
       " ('even', 'thousand', 'grams'),\n",
       " ('thousand', 'grams', 'depending'),\n",
       " ('grams', 'depending', 'on'),\n",
       " ('depending', 'on', 'what'),\n",
       " ('on', 'what', 'they'),\n",
       " ('what', 'they', 'are'),\n",
       " ('they', 'are', 'trying'),\n",
       " ('are', 'trying', 'to'),\n",
       " ('trying', 'to', 'go')]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's create my trigrams\n",
    "my_tri_grams = ngrams(my_tokens,3)\n",
    "list(my_tri_grams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Similar things could have been done for 4,5,6 grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# One could have written a function and find the ngrams\n",
    "def ngrams_constructor(text,n):\n",
    "    return ngrams(text.split(),n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "biGram = ngrams_constructor('Sidy is a a free black man loved by Jesus Chris',2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Sidy', 'is'),\n",
       " ('is', 'a'),\n",
       " ('a', 'a'),\n",
       " ('a', 'free'),\n",
       " ('free', 'black'),\n",
       " ('black', 'man'),\n",
       " ('man', 'loved'),\n",
       " ('loved', 'by'),\n",
       " ('by', 'Jesus'),\n",
       " ('Jesus', 'Chris')]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(biGram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "triGram = ngrams_constructor('Sidy is a a free black man loved by Jesus Chris',3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Sidy', 'is', 'a'),\n",
       " ('is', 'a', 'a'),\n",
       " ('a', 'a', 'free'),\n",
       " ('a', 'free', 'black'),\n",
       " ('free', 'black', 'man'),\n",
       " ('black', 'man', 'loved'),\n",
       " ('man', 'loved', 'by'),\n",
       " ('loved', 'by', 'Jesus'),\n",
       " ('by', 'Jesus', 'Chris')]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(triGram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentiment(polarity=-0.35, subjectivity=0.6000000000000001)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Sentiment(polarity=-0.35, subjectivity=0.6000000000000001)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Textblob does not consider words that are made of one letter in the \n",
    "# sentiment analysis\n",
    "print TextBlob('not a good person').sentiment\n",
    "TextBlob('not good person').sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TextBlob(\"Hello friends\")"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# language identification\n",
    "TextBlob('Hola amigos').translate(from_lang='auto', to='en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# The polarity of a word is always between -1 and 1\n",
    "# Subejctivity is between 0 and 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Yelp review analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import metrics\n",
    "from textblob import TextBlob, Word\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "yelp = pd.read_csv('yelp.csv')\n",
    "yelp.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 10000 entries, 0 to 9999\n",
      "Data columns (total 10 columns):\n",
      "business_id    10000 non-null object\n",
      "date           10000 non-null object\n",
      "review_id      10000 non-null object\n",
      "stars          10000 non-null int64\n",
      "text           10000 non-null object\n",
      "type           10000 non-null object\n",
      "user_id        10000 non-null object\n",
      "cool           10000 non-null int64\n",
      "useful         10000 non-null int64\n",
      "funny          10000 non-null int64\n",
      "dtypes: int64(4), object(6)\n",
      "memory usage: 781.3+ KB\n"
     ]
    }
   ],
   "source": [
    "yelp.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>stars</th>\n",
       "      <th>cool</th>\n",
       "      <th>useful</th>\n",
       "      <th>funny</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>10000.000000</td>\n",
       "      <td>10000.000000</td>\n",
       "      <td>10000.000000</td>\n",
       "      <td>10000.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>3.777500</td>\n",
       "      <td>0.876800</td>\n",
       "      <td>1.409300</td>\n",
       "      <td>0.701300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1.214636</td>\n",
       "      <td>2.067861</td>\n",
       "      <td>2.336647</td>\n",
       "      <td>1.907942</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>3.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>4.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>5.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>5.000000</td>\n",
       "      <td>77.000000</td>\n",
       "      <td>76.000000</td>\n",
       "      <td>57.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              stars          cool        useful         funny\n",
       "count  10000.000000  10000.000000  10000.000000  10000.000000\n",
       "mean       3.777500      0.876800      1.409300      0.701300\n",
       "std        1.214636      2.067861      2.336647      1.907942\n",
       "min        1.000000      0.000000      0.000000      0.000000\n",
       "25%        3.000000      0.000000      0.000000      0.000000\n",
       "50%        4.000000      0.000000      1.000000      0.000000\n",
       "75%        5.000000      1.000000      2.000000      1.000000\n",
       "max        5.000000     77.000000     76.000000     57.000000"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yelp.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cool</th>\n",
       "      <th>useful</th>\n",
       "      <th>funny</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>stars</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.576769</td>\n",
       "      <td>1.604806</td>\n",
       "      <td>1.056075</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.719525</td>\n",
       "      <td>1.563107</td>\n",
       "      <td>0.875944</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.788501</td>\n",
       "      <td>1.306639</td>\n",
       "      <td>0.694730</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.954623</td>\n",
       "      <td>1.395916</td>\n",
       "      <td>0.670448</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.944261</td>\n",
       "      <td>1.381780</td>\n",
       "      <td>0.608631</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           cool    useful     funny\n",
       "stars                              \n",
       "1      0.576769  1.604806  1.056075\n",
       "2      0.719525  1.563107  0.875944\n",
       "3      0.788501  1.306639  0.694730\n",
       "4      0.954623  1.395916  0.670448\n",
       "5      0.944261  1.381780  0.608631"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_by_group = yelp.groupby('stars').mean()\n",
    "mean_by_group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cool</th>\n",
       "      <th>useful</th>\n",
       "      <th>funny</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>cool</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.743329</td>\n",
       "      <td>-0.944939</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>useful</th>\n",
       "      <td>-0.743329</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.894506</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>funny</th>\n",
       "      <td>-0.944939</td>\n",
       "      <td>0.894506</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            cool    useful     funny\n",
       "cool    1.000000 -0.743329 -0.944939\n",
       "useful -0.743329  1.000000  0.894506\n",
       "funny  -0.944939  0.894506  1.000000"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_by_group.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4086L,)\n"
     ]
    }
   ],
   "source": [
    "#create a new DataFrame that only contains the 5-star and 1-star reviews\n",
    "yelp_best_worst = yelp[(yelp.stars==5) | (yelp.stars==1)]\n",
    "\n",
    "yelp_best_worst.reset_index(drop=True, inplace=True) # reset the indices. And instead of \n",
    "# creating another data frame, let's just do it inplace\n",
    "\n",
    "x = yelp_best_worst.text #reviews\n",
    "y = yelp_best_worst.stars #ratings\n",
    "# print x to look at x\n",
    "# print y to take a look at\n",
    "print (x.shape)\n",
    "\n",
    "#split into training and test sets\n",
    "x_train, x_test, y_train, y_test = train_test_split(x,y,random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0       My wife took me here on my birthday for breakf...\n",
      "1       I have no idea why some people give bad review...\n",
      "2       Rosie, Dakota, and I LOVE Chaparral Dog Park!!...\n",
      "3       General Manager Scott Petello is a good egg!!!...\n",
      "4       Drop what you're doing and drive here. After I...\n",
      "5       Nobuo shows his unique talents with everything...\n",
      "6       The oldish man who owns the store is as sweet ...\n",
      "7       Wonderful Vietnamese sandwich shoppe. Their ba...\n",
      "8       They have a limited time thing going on right ...\n",
      "9       okay this is the best place EVER! i grew up sh...\n",
      "10      This place shouldn't even be reviewed - becaus...\n",
      "11      first time my friend and I went there... it wa...\n",
      "12      U can go there n check the car out. If u wanna...\n",
      "13      I love this place! I have been coming here for...\n",
      "14      I love love LOVE this place. My boss (who is i...\n",
      "15      Disclaimer: Like many of you, I am a sucker fo...\n",
      "16      Disgusting!  Had a Groupon so my daughter and ...\n",
      "17      Never having dealt with a Discount Tire in Pho...\n",
      "18      I've eaten here many times, but none as bad as...\n",
      "19      (Un)fortunately for me, lux is close to my hou...\n",
      "20      Fred M. pretty much said what I would say, so ...\n",
      "21      Alright, I have been away from Yelp for quite ...\n",
      "22      This restaurant is incredible, and has the bes...\n",
      "23      I have always been a fan of Burlington's deals...\n",
      "24      Another night meeting friends here.  I have to...\n",
      "25      Not busy at all but took nearly 45 min to get ...\n",
      "26      This an incredible church that embraces the pr...\n",
      "27      This is our favorite breakfast place. The food...\n",
      "28      I had looked at several invitation websites al...\n",
      "29      Yikes, reading other reviews I realize my bad ...\n",
      "                              ...                        \n",
      "4056    I have a fond place in my heart for this estab...\n",
      "4057    Cork is an enigma.\\n\\nWhat makes it enigmatic ...\n",
      "4058    Went to Yogurt Kingdom for the first time toni...\n",
      "4059    I find it hilarious that someone would referen...\n",
      "4060                                      LOVE Five Guys!\n",
      "4061    This is a great Mexican food restaurant. I eat...\n",
      "4062    \"Hipster,Trendy\" ????-I think NOT !!!! Very di...\n",
      "4063    \"So Jimmy, tell the class what you saw at Swee...\n",
      "4064    Standard Mexican fare - but quite delicious.  ...\n",
      "4065    My profile says....\\n\\nMy Last Meal On Earth: ...\n",
      "4066    Treats: We tried the cookies (chocolate chip a...\n",
      "4067    I first joined 24 hr fitness about a year ago,...\n",
      "4068    Leah, the trainer, at Dog House Training Acade...\n",
      "4069    This place is super cute lunch joint.  I had t...\n",
      "4070    The staff is great, the food is great, even th...\n",
      "4071    Wow!  Went on a Sunday around 11am - busy but ...\n",
      "4072    When I lived in Phoenix, I was a regular at Fe...\n",
      "4073    Why did I wait so long to try this neighborhoo...\n",
      "4074    This is the place for a fabulos breakfast!! I ...\n",
      "4075    Highly recommend. This is my second time here ...\n",
      "4076    5 stars for the great $5 happy hour specials. ...\n",
      "4077    We brought the entire family to Giuseppe's las...\n",
      "4078    Went last night to Whore Foods to get basics t...\n",
      "4079    The food is delicious.  The service:  discrimi...\n",
      "4080    Great food and service! Country food at its best!\n",
      "4081    Yes I do rock the hipster joints.  I dig this ...\n",
      "4082    Only 4 stars? \\n\\n(A few notes: The folks that...\n",
      "4083    I'm not normally one to jump at reviewing a ch...\n",
      "4084    Let's see...what is there NOT to like about Su...\n",
      "4085    4-5 locations.. all 4.5 star average.. I think...\n",
      "Name: text, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'My wife took me here on my birthday for breakfast and it was excellent.  The weather was perfect which made sitting outside overlooking their grounds an absolute pleasure.  Our waitress was excellent and our food arrived quickly on the semi-busy Saturday morning.  It looked like the place fills up pretty quickly so the earlier you get here the better.\\n\\nDo yourself a favor and get their Bloody Mary.  It was phenomenal and simply the best I\\'ve ever had.  I\\'m pretty sure they only use ingredients from their garden and blend them fresh when you order it.  It was amazing.\\n\\nWhile EVERYTHING on the menu looks excellent, I had the white truffle scrambled eggs vegetable skillet and it was tasty and delicious.  It came with 2 pieces of their griddled bread with was amazing and it absolutely made the meal complete.  It was the best \"toast\" I\\'ve ever had.\\n\\nAnyway, I can\\'t wait to go back!'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 5773)\t1\n",
      "  (0, 10362)\t2\n",
      "  (0, 12465)\t1\n",
      "  (0, 10069)\t1\n",
      "  (0, 10180)\t1\n",
      "  (0, 16612)\t2\n",
      "  (0, 4631)\t1\n",
      "  (0, 9578)\t1\n",
      "  (0, 15093)\t1\n",
      "  (0, 11186)\t1\n",
      "  (0, 136)\t1\n",
      "  (0, 4809)\t1\n",
      "  (0, 15136)\t1\n",
      "  (0, 10413)\t2\n",
      "  (0, 16195)\t1\n",
      "  (0, 15834)\t1\n",
      "  (0, 12514)\t2\n",
      "  (0, 2789)\t1\n",
      "  (0, 14838)\t1\n",
      "  (0, 10286)\t2\n",
      "  (0, 3679)\t1\n",
      "  (0, 15032)\t2\n",
      "  (0, 1018)\t1\n",
      "  (0, 2286)\t2\n",
      "  (0, 1003)\t1\n",
      "  :\t:\n",
      "  (3063, 2312)\t1\n",
      "  (3063, 9318)\t1\n",
      "  (3063, 879)\t1\n",
      "  (3063, 10352)\t2\n",
      "  (3063, 15968)\t1\n",
      "  (3063, 7181)\t1\n",
      "  (3063, 15042)\t1\n",
      "  (3063, 5333)\t1\n",
      "  (3063, 8189)\t2\n",
      "  (3063, 1548)\t1\n",
      "  (3063, 9807)\t1\n",
      "  (3063, 2818)\t1\n",
      "  (3063, 2735)\t1\n",
      "  (3063, 14836)\t1\n",
      "  (3063, 6718)\t1\n",
      "  (3063, 16599)\t1\n",
      "  (3063, 6974)\t1\n",
      "  (3063, 14137)\t1\n",
      "  (3063, 5139)\t1\n",
      "  (3063, 4538)\t1\n",
      "  (3063, 10805)\t1\n",
      "  (3063, 14994)\t1\n",
      "  (3063, 9438)\t1\n",
      "  (3063, 16162)\t1\n",
      "  (3063, 6616)\t1\n"
     ]
    }
   ],
   "source": [
    "# use CountVetorizer to create document-term matrices from x_train and x_test\n",
    "vect = CountVectorizer()\n",
    "#Tokenize the documents and count the occurrences of token and \n",
    "#return them as a sparse matrix\n",
    "x_train_dtm = vect.fit_transform(x_train) # learn the vocabulary dictionary ad create term document matrix\n",
    "print (x_train_dtm)\n",
    "#print (x_train_dtm.shape)\n",
    "x_test_dtm= vect.transform(x_test)\n",
    "#print x_test_dtm\n",
    "#x_test_dtm.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1607    Looking a cutting edge, wanting the best for e...\n",
      "3409    Greatness in the form of food, just like the o...\n",
      "1751    The Flower Studio far exceeded my expectations...\n",
      "2275        So yummy! Strange combination but great place\n",
      "230     I've been hearing about these cheesecakes from...\n",
      "902     This has to be the worst restaurant in terms o...\n",
      "1865    I ate at Scramble last Friday and I have to sa...\n",
      "636     We decided to eat here on a whim. My husband g...\n",
      "2625    I LOVE BURRITO EXPRESS. My fiance has been goi...\n",
      "943     Just open.  I had the roast beef sandwich and ...\n",
      "1171    Cute busy place in Central Phoenix. Not hiding...\n",
      "1247    I'm a big fan of Silver Mine. I have been for ...\n",
      "200     I have now visited Herb n' Flavors several tim...\n",
      "891     I love to come here from time to time when I'm...\n",
      "443     Went to Fatburger with our family tonight and ...\n",
      "2497    This review pertains to carnitas, and as such ...\n",
      "1673    TIP #1 to Mesa-Gateway fliers: This is the ONL...\n",
      "745     If you like the stuck up Scottsdale vibe this ...\n",
      "1105    I'm sorry to be what seems to be the lone one ...\n",
      "3227    Unprofessional, disorganized, and extremely lo...\n",
      "1164    Bad music, slow service, disgusting overpriced...\n",
      "3896    I can't remember the name of the special salad...\n",
      "4001    Went here last night when on our last night st...\n",
      "3868    There is only one reason  why I shouldn't love...\n",
      "4071    Wow!  Went on a Sunday around 11am - busy but ...\n",
      "1767    Wow! The Penang Curry (chicken) was absolutely...\n",
      "936     This place is what Desert Ridge wishes it was!...\n",
      "1249    Andrea is absolutely wonderful. She's pet-sit ...\n",
      "2095    I am from Chicago - and the italian beef here ...\n",
      "3803    I was given a $100 gift card to use at Willow ...\n",
      "                              ...                        \n",
      "2867    Totally excited to try this place out, my gran...\n",
      "1533    Went here for the first time today.  Loved it....\n",
      "3266    Still a place that is unacceptable in my book-...\n",
      "407     I took my family here and this was a disappoin...\n",
      "137     So your going to Scottsdale via Paradise Valle...\n",
      "973     Very good place to eat.. I go here atleast 3 t...\n",
      "797     Just did take out.  Great experience.  Easy to...\n",
      "1808    my husband and i LOVE this place! \\n\\nno, we c...\n",
      "1094    Best Greek food I had in Arizona and excellent...\n",
      "1545              Best ribs in Arizona (besides my own)!!\n",
      "2346    The greatest community that I've ever been a p...\n",
      "3330    Love Belly Rubz!! At first little Oscar was ti...\n",
      "2970    A yummy Mexican Sunnyslope dive.  The oatmeal ...\n",
      "1540    Have been going to LGO since 2003 and have alw...\n",
      "3255    I have taken acting classes from Verve Studios...\n",
      "834     I have been going to the Matador since I was l...\n",
      "516     Fantastic donuts! Great selection! Coffee was ...\n",
      "2969    Thanks for helping me to find Valley Eyecare C...\n",
      "1291    Very good place for breakfast and their pies a...\n",
      "1156    What an awesome business! Friendly, knowledgab...\n",
      "220     HELLISH HELLISH SUMMER WEATHER (March thru Oct...\n",
      "3424    Love this spot - it's pretty close to the conv...\n",
      "3344    Found the Tuck Shop on my Urban Spoon AP and w...\n",
      "3685    I usually do not complain about bad food but t...\n",
      "3141    Wow, this place is still here? I went there as...\n",
      "2793    Honey jalapeño chicken lollipops and sweet pot...\n",
      "671                    probably my favorite restaurant :)\n",
      "3441    A philosophical elder of my profession commonl...\n",
      "3224    First, I'm sorry this review is lengthy, but i...\n",
      "3362    You speak Italian to me and provide mouth wate...\n",
      "Name: text, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print x_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>00</th>\n",
       "      <th>000</th>\n",
       "      <th>00a</th>\n",
       "      <th>00am</th>\n",
       "      <th>00pm</th>\n",
       "      <th>01</th>\n",
       "      <th>02</th>\n",
       "      <th>03</th>\n",
       "      <th>03342</th>\n",
       "      <th>04</th>\n",
       "      <th>...</th>\n",
       "      <th>zucchini</th>\n",
       "      <th>zuchinni</th>\n",
       "      <th>zumba</th>\n",
       "      <th>zupa</th>\n",
       "      <th>zuzu</th>\n",
       "      <th>zwiebel</th>\n",
       "      <th>zzed</th>\n",
       "      <th>éclairs</th>\n",
       "      <th>école</th>\n",
       "      <th>ém</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 16825 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   00  000  00a  00am  00pm  01  02  03  03342  04 ...  zucchini  zuchinni  \\\n",
       "0   0    0    0     0     0   0   0   0      0   0 ...         0         0   \n",
       "1   0    0    0     0     0   0   0   0      0   0 ...         0         0   \n",
       "2   0    0    0     0     0   0   0   0      0   0 ...         0         0   \n",
       "3   0    0    0     0     0   0   0   0      0   0 ...         0         0   \n",
       "4   0    0    0     0     0   0   0   0      0   0 ...         0         0   \n",
       "\n",
       "   zumba  zupa  zuzu  zwiebel  zzed  éclairs  école  ém  \n",
       "0      0     0     0        0     0        0      0   0  \n",
       "1      0     0     0        0     0        0      0   0  \n",
       "2      0     0     0        0     0        0      0   0  \n",
       "3      0     0     0        0     0        0      0   0  \n",
       "4      0     0     0        0     0        0      0   0  \n",
       "\n",
       "[5 rows x 16825 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf = pd.DataFrame(x_train_dtm.toarray(), columns=vect.get_feature_names())\n",
    "tf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2790    FILLY-B's!!!!!  only 8 reviews?? NINE now!!!\\n...\n",
       "725     My husband and I absolutely LOVE this restaura...\n",
       "1578    We went today after lunch. I got my usual of l...\n",
       "282     Totally dissapointed.  I had purchased a coupo...\n",
       "2024    Costco Travel - My husband and I recently retu...\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3064, 20838)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#don't lowercase\n",
    "vect = CountVectorizer(lowercase=False)\n",
    "x_train_dtm = vect.fit_transform(x_train)\n",
    "x_train_dtm.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3064, 169847)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# include 1-grams and 2-grams (an n-gram is N-grams is just all combinations of adjacent words \n",
    "# or letters of length n that you can find in your source text)\n",
    "vect = CountVectorizer(ngram_range=(1,2))\n",
    "x_train_dtm = vect.fit_transform(x_train)\n",
    "x_train_dtm.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'zone out', u'zone when', u'zones', u'zones dolls', u'zoning', u'zoning issues', u'zoo', u'zoo and', u'zoo is', u'zoo not', u'zoo the', u'zoo ve', u'zoyo', u'zoyo for', u'zucca', u'zucca appetizer', u'zucchini', u'zucchini and', u'zucchini bread', u'zucchini broccoli', u'zucchini carrots', u'zucchini fries', u'zucchini pieces', u'zucchini strips', u'zucchini veal', u'zucchini very', u'zucchini with', u'zuchinni', u'zuchinni again', u'zuchinni the', u'zumba', u'zumba class', u'zumba or', u'zumba yogalates', u'zupa', u'zupa flavors', u'zuzu', u'zuzu in', u'zuzu is', u'zuzu the', u'zwiebel', u'zwiebel kr\\xe4uter', u'zzed', u'zzed in', u'\\xe9clairs', u'\\xe9clairs napoleons', u'\\xe9cole', u'\\xe9cole len\\xf4tre', u'\\xe9m', u'\\xe9m all']\n"
     ]
    }
   ],
   "source": [
    "print (vect.get_feature_names()[-50:]) # The last 50 words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict the star rating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.918786692759\n"
     ]
    }
   ],
   "source": [
    "vect = CountVectorizer()\n",
    "\n",
    "x_train_dtm = vect.fit_transform(x_train)\n",
    "x_test_dtm = vect.transform(x_test)\n",
    "\n",
    "# Questions?\n",
    "# what is the difference between f\n",
    "#fit() : is used to generate learning model parameters from training data\n",
    "#transform() : parameters generated from fit() method,applied upon model to\n",
    "# generate transformed data set.\n",
    "# fit_transform() : combines fit() and transform() api on same data sets\n",
    "\n",
    "#Naive Bayes\n",
    "nb = MultinomialNB()\n",
    "nb.fit(x_train_dtm, y_train)\n",
    "y_pred_class = nb.predict(x_test_dtm)\n",
    "\n",
    "print (metrics.accuracy_score(y_test, y_pred_class))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# calculate null accuracy\n",
    "y_test_binary = np.where(y_test==5, 1, 0)\n",
    "max(y_test_binary.mean(), 1-y_test_binary.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#define a function that accepts a vectorizer and calculates the accuracy\n",
    "def tokenize_test(vect):\n",
    "    x_train_dtm = vect.fit_transform(x_train)\n",
    "    print ('Features: ', x_train_dtm.shape[1])\n",
    "    x_test_dtm = vect.transform(x_test)\n",
    "    nb = MultinomialNB()\n",
    "    nb.fit(x_train_dtm, y_train)\n",
    "    y_pred_class = nb.predict(x_test_dtm)\n",
    "    print ('Accuracy: ', metrics.accuracy_score(y_test, y_pred_class))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Features: ', 169847)\n",
      "('Accuracy: ', 0.85420743639921726)\n"
     ]
    }
   ],
   "source": [
    "#include 1-grams and 2-grams\n",
    "vect = CountVectorizer(ngram_range=(1,2))\n",
    "tokenize_test(vect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Features: ', 16825)\n",
      "('Accuracy: ', 0.91878669275929548)\n"
     ]
    }
   ],
   "source": [
    "vect = CountVectorizer()\n",
    "tokenize_test(vect)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stopword removal\n",
    "- Stopwords are used words that are widely used in a language.\n",
    "- To find the list of stopwords in a given language, one can easily type \n",
    "    - import nltk\n",
    "    - from nltk.corpus import stopwords\n",
    "    - set(stopwords.words('the language = 'english', ;french'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Let's suppose that you have obtain the set of stopwords. And suppose\n",
    "# that you want to add something new into the your list words. To do so,\n",
    "# write a function that will allow you achieve your goals.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# one can respond to this in two ways. Let's\n",
    "my_test_corpus = 'I am a Sidy. A free black man. Believe it or not; it is true'\n",
    "# Do it by words and by sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Features: ', 16528)\n",
      "('Accuracy: ', 0.91585127201565553)\n"
     ]
    }
   ],
   "source": [
    "#remove English stop words\n",
    "vect = CountVectorizer(stop_words='english')\n",
    "tokenize_test(vect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "frozenset(['all', 'six', 'less', 'being', 'indeed', 'over', 'move', 'anyway', 'four', 'not', 'own', 'through', 'yourselves', 'fify', 'where', 'mill', 'only', 'find', 'before', 'one', 'whose', 'system', 'how', 'somewhere', 'with', 'thick', 'show', 'had', 'enough', 'should', 'to', 'must', 'whom', 'seeming', 'under', 'ours', 'has', 'might', 'thereafter', 'latterly', 'do', 'them', 'his', 'around', 'than', 'get', 'very', 'de', 'none', 'cannot', 'every', 'whether', 'they', 'front', 'during', 'thus', 'now', 'him', 'nor', 'name', 'several', 'hereafter', 'always', 'who', 'cry', 'whither', 'this', 'someone', 'either', 'each', 'become', 'thereupon', 'sometime', 'side', 'two', 'therein', 'twelve', 'because', 'often', 'ten', 'our', 'eg', 'some', 'back', 'up', 'go', 'namely', 'towards', 'are', 'further', 'beyond', 'ourselves', 'yet', 'out', 'even', 'will', 'what', 'still', 'for', 'bottom', 'mine', 'since', 'please', 'forty', 'per', 'its', 'everything', 'behind', 'un', 'above', 'between', 'it', 'neither', 'seemed', 'ever', 'across', 'she', 'somehow', 'be', 'we', 'full', 'never', 'sixty', 'however', 'here', 'otherwise', 'were', 'whereupon', 'nowhere', 'although', 'found', 'alone', 're', 'along', 'fifteen', 'by', 'both', 'about', 'last', 'would', 'anything', 'via', 'many', 'could', 'thence', 'put', 'against', 'keep', 'etc', 'amount', 'became', 'ltd', 'hence', 'onto', 'or', 'con', 'among', 'already', 'co', 'afterwards', 'formerly', 'within', 'seems', 'into', 'others', 'while', 'whatever', 'except', 'down', 'hers', 'everyone', 'done', 'least', 'another', 'whoever', 'moreover', 'couldnt', 'throughout', 'anyhow', 'yourself', 'three', 'from', 'her', 'few', 'together', 'top', 'there', 'due', 'been', 'next', 'anyone', 'eleven', 'much', 'call', 'therefore', 'interest', 'then', 'thru', 'themselves', 'hundred', 'was', 'sincere', 'empty', 'more', 'himself', 'elsewhere', 'mostly', 'on', 'fire', 'am', 'becoming', 'hereby', 'amongst', 'else', 'part', 'everywhere', 'too', 'herself', 'former', 'those', 'he', 'me', 'myself', 'made', 'twenty', 'these', 'bill', 'cant', 'us', 'until', 'besides', 'nevertheless', 'below', 'anywhere', 'nine', 'can', 'of', 'toward', 'my', 'something', 'and', 'whereafter', 'whenever', 'give', 'almost', 'wherever', 'is', 'describe', 'beforehand', 'herein', 'an', 'as', 'itself', 'at', 'have', 'in', 'seem', 'whence', 'ie', 'any', 'fill', 'again', 'hasnt', 'inc', 'thereby', 'thin', 'no', 'perhaps', 'latter', 'meanwhile', 'when', 'detail', 'same', 'wherein', 'beside', 'also', 'that', 'other', 'take', 'which', 'becomes', 'you', 'if', 'nobody', 'see', 'though', 'may', 'after', 'upon', 'most', 'hereupon', 'eight', 'but', 'serious', 'nothing', 'such', 'your', 'why', 'a', 'off', 'whereby', 'third', 'i', 'whole', 'noone', 'sometimes', 'well', 'amoungst', 'yours', 'their', 'rather', 'without', 'so', 'five', 'the', 'first', 'whereas', 'once'])\n"
     ]
    }
   ],
   "source": [
    "# set of stop words\n",
    "print (vect.get_stop_words())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Features: ', 100)\n",
      "('Accuracy: ', 0.86986301369863017)\n"
     ]
    }
   ],
   "source": [
    "#max_features\n",
    "vect = CountVectorizer(stop_words='english', max_features=100)\n",
    "tokenize_test(vect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'amazing', u'area', u'atmosphere', u'awesome', u'bad', u'bar', u'best', u'better', u'big', u'came', u'cheese', u'chicken', u'clean', u'coffee', u'come', u'day', u'definitely', u'delicious', u'did', u'didn', u'dinner', u'don', u'eat', u'excellent', u'experience', u'favorite', u'feel', u'food', u'free', u'fresh', u'friendly', u'friends', u'going', u'good', u'got', u'great', u'happy', u'home', u'hot', u'hour', u'just', u'know', u'like', u'little', u'll', u'location', u'long', u'looking', u'lot', u'love', u'lunch', u'make', u'meal', u'menu', u'minutes', u'need', u'new', u'nice', u'night', u'order', u'ordered', u'people', u'perfect', u'phoenix', u'pizza', u'place', u'pretty', u'prices', u'really', u'recommend', u'restaurant', u'right', u'said', u'salad', u'sandwich', u'sauce', u'say', u'service', u'staff', u'store', u'sure', u'table', u'thing', u'things', u'think', u'time', u'times', u'took', u'town', u'tried', u'try', u've', u'wait', u'want', u'way', u'went', u'wine', u'work', u'worth', u'years']\n"
     ]
    }
   ],
   "source": [
    "print(vect.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Features: ', 100000)\n",
      "('Accuracy: ', 0.88551859099804309)\n"
     ]
    }
   ],
   "source": [
    "#From here\n",
    "vect = CountVectorizer(ngram_range=(1,2), max_features=100000)\n",
    "tokenize_test(vect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Features: ', 43957)\n",
      "('Accuracy: ', 0.93248532289628183)\n"
     ]
    }
   ],
   "source": [
    "#min_df sets the minimum document frequency allowed when creating vocab\n",
    "vect = CountVectorizer(ngram_range=(1,2), min_df=2)\n",
    "tokenize_test(vect)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TextBlob\n",
    "- is a Python (2 and 3) library for processing textual data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "My wife took me here on my birthday for breakfast and it was excellent.  The weather was perfect which made sitting outside overlooking their grounds an absolute pleasure.  Our waitress was excellent and our food arrived quickly on the semi-busy Saturday morning.  It looked like the place fills up pretty quickly so the earlier you get here the better.\n",
      "\n",
      "Do yourself a favor and get their Bloody Mary.  It was phenomenal and simply the best I've ever had.  I'm pretty sure they only use ingredients from their garden and blend them fresh when you order it.  It was amazing.\n",
      "\n",
      "While EVERYTHING on the menu looks excellent, I had the white truffle scrambled eggs vegetable skillet and it was tasty and delicious.  It came with 2 pieces of their griddled bread with was amazing and it absolutely made the meal complete.  It was the best \"toast\" I've ever had.\n",
      "\n",
      "Anyway, I can't wait to go back!\n"
     ]
    }
   ],
   "source": [
    "print (yelp_best_worst.text[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "review = TextBlob(yelp_best_worst.text[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WordList(['My', 'wife', 'took', 'me', 'here', 'on', 'my', 'birthday', 'for', 'breakfast', 'and', 'it', 'was', 'excellent', 'The', 'weather', 'was', 'perfect', 'which', 'made', 'sitting', 'outside', 'overlooking', 'their', 'grounds', 'an', 'absolute', 'pleasure', 'Our', 'waitress', 'was', 'excellent', 'and', 'our', 'food', 'arrived', 'quickly', 'on', 'the', 'semi-busy', 'Saturday', 'morning', 'It', 'looked', 'like', 'the', 'place', 'fills', 'up', 'pretty', 'quickly', 'so', 'the', 'earlier', 'you', 'get', 'here', 'the', 'better', 'Do', 'yourself', 'a', 'favor', 'and', 'get', 'their', 'Bloody', 'Mary', 'It', 'was', 'phenomenal', 'and', 'simply', 'the', 'best', 'I', \"'ve\", 'ever', 'had', 'I', \"'m\", 'pretty', 'sure', 'they', 'only', 'use', 'ingredients', 'from', 'their', 'garden', 'and', 'blend', 'them', 'fresh', 'when', 'you', 'order', 'it', 'It', 'was', 'amazing', 'While', 'EVERYTHING', 'on', 'the', 'menu', 'looks', 'excellent', 'I', 'had', 'the', 'white', 'truffle', 'scrambled', 'eggs', 'vegetable', 'skillet', 'and', 'it', 'was', 'tasty', 'and', 'delicious', 'It', 'came', 'with', '2', 'pieces', 'of', 'their', 'griddled', 'bread', 'with', 'was', 'amazing', 'and', 'it', 'absolutely', 'made', 'the', 'meal', 'complete', 'It', 'was', 'the', 'best', 'toast', 'I', \"'ve\", 'ever', 'had', 'Anyway', 'I', 'ca', \"n't\", 'wait', 'to', 'go', 'back'])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "review.words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Sentence(\"My wife took me here on my birthday for breakfast and it was excellent.\"),\n",
       " Sentence(\"The weather was perfect which made sitting outside overlooking their grounds an absolute pleasure.\"),\n",
       " Sentence(\"Our waitress was excellent and our food arrived quickly on the semi-busy Saturday morning.\"),\n",
       " Sentence(\"It looked like the place fills up pretty quickly so the earlier you get here the better.\"),\n",
       " Sentence(\"Do yourself a favor and get their Bloody Mary.\"),\n",
       " Sentence(\"It was phenomenal and simply the best I've ever had.\"),\n",
       " Sentence(\"I'm pretty sure they only use ingredients from their garden and blend them fresh when you order it.\"),\n",
       " Sentence(\"It was amazing.\"),\n",
       " Sentence(\"While EVERYTHING on the menu looks excellent, I had the white truffle scrambled eggs vegetable skillet and it was tasty and delicious.\"),\n",
       " Sentence(\"It came with 2 pieces of their griddled bread with was amazing and it absolutely made the meal complete.\"),\n",
       " Sentence(\"It was the best \"toast\" I've ever had.\"),\n",
       " Sentence(\"Anyway, I can't wait to go back!\")]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "review.sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TextBlob(\"my wife took me here on my birthday for breakfast and it was excellent.  the weather was perfect which made sitting outside overlooking their grounds an absolute pleasure.  our waitress was excellent and our food arrived quickly on the semi-busy saturday morning.  it looked like the place fills up pretty quickly so the earlier you get here the better.\n",
       "\n",
       "do yourself a favor and get their bloody mary.  it was phenomenal and simply the best i've ever had.  i'm pretty sure they only use ingredients from their garden and blend them fresh when you order it.  it was amazing.\n",
       "\n",
       "while everything on the menu looks excellent, i had the white truffle scrambled eggs vegetable skillet and it was tasty and delicious.  it came with 2 pieces of their griddled bread with was amazing and it absolutely made the meal complete.  it was the best \"toast\" i've ever had.\n",
       "\n",
       "anyway, i can't wait to go back!\")"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "review.lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stemming and lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'my', u'wife', u'took', u'me', u'here', u'on', u'my', u'birthday', u'for', u'breakfast', u'and', u'it', u'was', u'excel', u'the', u'weather', u'was', u'perfect', u'which', u'made', u'sit', u'outsid', u'overlook', u'their', u'ground', u'an', u'absolut', u'pleasur', u'our', u'waitress', u'was', u'excel', u'and', u'our', u'food', u'arriv', u'quick', u'on', u'the', u'semi-busi', u'saturday', u'morn', u'it', u'look', u'like', u'the', u'place', u'fill', u'up', u'pretti', u'quick', u'so', u'the', u'earlier', u'you', u'get', u'here', u'the', u'better', u'do', u'yourself', u'a', u'favor', u'and', u'get', u'their', u'bloodi', u'mari', u'it', u'was', u'phenomen', u'and', u'simpli', u'the', u'best', u'i', u've', u'ever', u'had', u'i', u\"'m\", u'pretti', u'sure', u'they', u'onli', u'use', u'ingredi', u'from', u'their', u'garden', u'and', u'blend', u'them', u'fresh', u'when', u'you', u'order', u'it', u'it', u'was', u'amaz', u'while', u'everyth', u'on', u'the', u'menu', u'look', u'excel', u'i', u'had', u'the', u'white', u'truffl', u'scrambl', u'egg', u'veget', u'skillet', u'and', u'it', u'was', u'tasti', u'and', u'delici', u'it', u'came', u'with', u'2', u'piec', u'of', u'their', u'griddl', u'bread', u'with', u'was', u'amaz', u'and', u'it', u'absolut', u'made', u'the', u'meal', u'complet', u'it', u'was', u'the', u'best', u'toast', u'i', u've', u'ever', u'had', u'anyway', u'i', u'ca', u\"n't\", u'wait', u'to', u'go', u'back']\n"
     ]
    }
   ],
   "source": [
    "stemmer = SnowballStemmer('english')\n",
    "print ([stemmer.stem(word) for word in review.words])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['My', 'wife', 'took', 'me', 'here', 'on', 'my', 'birthday', 'for', 'breakfast', 'and', 'it', u'wa', 'excellent', 'The', 'weather', u'wa', 'perfect', 'which', 'made', 'sitting', 'outside', 'overlooking', 'their', u'ground', 'an', 'absolute', 'pleasure', 'Our', 'waitress', u'wa', 'excellent', 'and', 'our', 'food', 'arrived', 'quickly', 'on', 'the', 'semi-busy', 'Saturday', 'morning', 'It', 'looked', 'like', 'the', 'place', u'fill', 'up', 'pretty', 'quickly', 'so', 'the', 'earlier', 'you', 'get', 'here', 'the', 'better', 'Do', 'yourself', 'a', 'favor', 'and', 'get', 'their', 'Bloody', 'Mary', 'It', u'wa', 'phenomenal', 'and', 'simply', 'the', 'best', 'I', \"'ve\", 'ever', 'had', 'I', \"'m\", 'pretty', 'sure', 'they', 'only', 'use', u'ingredient', 'from', 'their', 'garden', 'and', 'blend', 'them', 'fresh', 'when', 'you', 'order', 'it', 'It', u'wa', 'amazing', 'While', 'EVERYTHING', 'on', 'the', 'menu', u'look', 'excellent', 'I', 'had', 'the', 'white', 'truffle', 'scrambled', u'egg', 'vegetable', 'skillet', 'and', 'it', u'wa', 'tasty', 'and', 'delicious', 'It', 'came', 'with', '2', u'piece', 'of', 'their', 'griddled', 'bread', 'with', u'wa', 'amazing', 'and', 'it', 'absolutely', 'made', 'the', 'meal', 'complete', 'It', u'wa', 'the', 'best', 'toast', 'I', \"'ve\", 'ever', 'had', 'Anyway', 'I', 'ca', \"n't\", 'wait', 'to', 'go', 'back']\n"
     ]
    }
   ],
   "source": [
    "print ([word.lemmatize() for word in review.words])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['My', 'wife', u'take', 'me', 'here', 'on', 'my', 'birthday', 'for', 'breakfast', 'and', 'it', u'be', 'excellent', 'The', 'weather', u'be', 'perfect', 'which', u'make', u'sit', 'outside', u'overlook', 'their', u'ground', 'an', 'absolute', 'pleasure', 'Our', 'waitress', u'be', 'excellent', 'and', 'our', 'food', u'arrive', 'quickly', 'on', 'the', 'semi-busy', 'Saturday', 'morning', 'It', u'look', 'like', 'the', 'place', u'fill', 'up', 'pretty', 'quickly', 'so', 'the', 'earlier', 'you', 'get', 'here', 'the', 'better', 'Do', 'yourself', 'a', 'favor', 'and', 'get', 'their', 'Bloody', 'Mary', 'It', u'be', 'phenomenal', 'and', 'simply', 'the', 'best', 'I', \"'ve\", 'ever', u'have', 'I', \"'m\", 'pretty', 'sure', 'they', 'only', 'use', 'ingredients', 'from', 'their', 'garden', 'and', 'blend', 'them', 'fresh', 'when', 'you', 'order', 'it', 'It', u'be', u'amaze', 'While', 'EVERYTHING', 'on', 'the', 'menu', u'look', 'excellent', 'I', u'have', 'the', 'white', 'truffle', u'scramble', u'egg', 'vegetable', 'skillet', 'and', 'it', u'be', 'tasty', 'and', 'delicious', 'It', u'come', 'with', '2', u'piece', 'of', 'their', u'griddle', 'bread', 'with', u'be', u'amaze', 'and', 'it', 'absolutely', u'make', 'the', 'meal', 'complete', 'It', u'be', 'the', 'best', 'toast', 'I', \"'ve\", 'ever', u'have', 'Anyway', 'I', 'ca', \"n't\", 'wait', 'to', 'go', 'back']\n"
     ]
    }
   ],
   "source": [
    "#assume every word is a verb\n",
    "print ([word.lemmatize(pos='v') for word in review.words])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def split_into_lemmas(text):\n",
    "    text = unicode(text, 'utf-8').lower() #Python 2\n",
    "    #text = text.lower() #Python 3\n",
    "    words = TextBlob(text).words\n",
    "    #return [word.lemmatize() for word in words]\n",
    "    return [stemmer.stem(word) for word in words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Features: ', 13273)\n",
      "('Accuracy: ', 0.92465753424657537)\n"
     ]
    }
   ],
   "source": [
    "#split review text into lemmas rather than into words (default)\n",
    "vect = CountVectorizer(analyzer=split_into_lemmas)\n",
    "tokenize_test(vect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'yuuuuummmmmyyi', u'yuuuuuuum', u'yuyuyummi', u'yuzu', u'z', u'z-grill', u'z11', u'zach', u'zam', u'zanella', u'zankou', u'zappo', u'zatsiki', u'zen', u'zen-lik', u'zero', u'zero-star', u'zest', u'zexperi', u'zha', u'zhou', u'zia', u'zilch', u'zin', u'zinburg', u'zinburgergeist', u'zinc', u'zinfandel', u'zing', u'zip', u'zipcar', u'zipp', u'zipper', u'ziti', u'zoe', u'zombi', u'zone', u'zoo', u'zoyo', u'zucca', u'zucchini', u'zuchinni', u'zumba', u'zupa', u'zuzu', u'zwiebel-kr\\xe4ut', u'zzed', u'\\xe9clair', u'\\xe9cole', u'\\xe9m']\n"
     ]
    }
   ],
   "source": [
    "print (vect.get_feature_names()[-50:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using TF-IDF to summarize a Yelp review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 28881)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#create a document-term matrix using TF-ID\n",
    "vect = TfidfVectorizer(stop_words='english')\n",
    "dtm = vect.fit_transform(yelp.text)\n",
    "features = vect.get_feature_names()\n",
    "dtm.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def summarize():\n",
    "    \n",
    "    #choose a random review that is at least 300 characters\n",
    "    review_length = 0\n",
    "    while review_length < 300:\n",
    "        review_id = np.random.randint(0, len(yelp))\n",
    "        review_text = unicode(yelp.text[review_id], 'utf-8') #Python 2\n",
    "        #review_text = str(yelp.text[review_id]) #Python3\n",
    "        review_length = len(review_text)\n",
    "        \n",
    "    #create a dictionary of words and their TF-IDF scores\n",
    "    word_scores = {}\n",
    "    for word in TextBlob(review_text).words:\n",
    "        word = word.lower()\n",
    "        if word in features:\n",
    "            word_scores[word] = dtm[review_id, features.index(word)]\n",
    "            \n",
    "    #print words with the top 5 TF-IDF scores\n",
    "    print ('TOP SCORING WORDS:')\n",
    "    top_scores = sorted(word_scores.items(), key=lambda x: x[1], reverse=True)[:5]\n",
    "    for word, score in top_scores:\n",
    "        print (word)\n",
    "        \n",
    "    #print the review\n",
    "    print ('\\n' + review_text) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TOP SCORING WORDS:\n",
      "bowl\n",
      "smashing\n",
      "soba\n",
      "circus\n",
      "facing\n",
      "\n",
      "I freakin love this place. My favorite thing is to sit and eat facing the counter and watch new people come in and get all confused. Now that's just funny. My first time I was the same way, like what the hell do I do here. Now I'm a pro. Stack it deep and use another bowl for smashing, Soba noodles piled so high it looks like a circus act getting the bowl to the cook. Mmmm....good.\n"
     ]
    }
   ],
   "source": [
    "summarize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "- Aims to sense people's mood based on the text they write.\n",
    "- Can be done when the text is quantifiable.\n",
    "- Sentiment can be positive or negative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "My wife took me here on my birthday for breakfast and it was excellent.  The weather was perfect which made sitting outside overlooking their grounds an absolute pleasure.  Our waitress was excellent and our food arrived quickly on the semi-busy Saturday morning.  It looked like the place fills up pretty quickly so the earlier you get here the better.\n",
      "\n",
      "Do yourself a favor and get their Bloody Mary.  It was phenomenal and simply the best I've ever had.  I'm pretty sure they only use ingredients from their garden and blend them fresh when you order it.  It was amazing.\n",
      "\n",
      "While EVERYTHING on the menu looks excellent, I had the white truffle scrambled eggs vegetable skillet and it was tasty and delicious.  It came with 2 pieces of their griddled bread with was amazing and it absolutely made the meal complete.  It was the best \"toast\" I've ever had.\n",
      "\n",
      "Anyway, I can't wait to go back!\n"
     ]
    }
   ],
   "source": [
    "print (review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our server Gary was awesome. Food was amazing...an experience.\n",
      "This was absolutely horrible. I got the supreme pizza with the mystery meats.  I threw it in the trash. I will wait until I get to my destination to eat. Horrible!!!\n"
     ]
    }
   ],
   "source": [
    "max_i = 0\n",
    "max_polarity = -float('inf')\n",
    "\n",
    "min_i = 0\n",
    "min_polarity = float('inf')\n",
    "\n",
    "for i in range(len(yelp_best_worst.text)):\n",
    "    review_text = unicode(yelp_best_worst.text[i], 'utf-8') #Python 2\n",
    "    #review_text = str(yelp_best_worst.text[i]) #Python3\n",
    "    this_polarity = TextBlob(review_text).sentiment.polarity\n",
    "    \n",
    "    if this_polarity > max_polarity:\n",
    "        max_i = i\n",
    "        max_polarity = this_polarity\n",
    "        \n",
    "    if this_polarity < min_polarity:\n",
    "        min_i = i\n",
    "        min_polarity = this_polarity\n",
    "\n",
    "print (TextBlob(yelp_best_worst.text[max_i]))\n",
    "print (TextBlob(yelp_best_worst.text[min_i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.402469135802\n",
      "1.0\n",
      "-1.0\n"
     ]
    }
   ],
   "source": [
    "#polarity ranges from -1 (most negative) to 1 (most positive)\n",
    "print(review.sentiment.polarity)\n",
    "print(max_polarity)\n",
    "print(min_polarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#understanding the apply method\n",
    "yelp['length'] = yelp.text.apply(len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>business_id</th>\n",
       "      <th>date</th>\n",
       "      <th>review_id</th>\n",
       "      <th>stars</th>\n",
       "      <th>text</th>\n",
       "      <th>type</th>\n",
       "      <th>user_id</th>\n",
       "      <th>cool</th>\n",
       "      <th>useful</th>\n",
       "      <th>funny</th>\n",
       "      <th>length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9yKzy9PApeiPPOUJEtnvkg</td>\n",
       "      <td>2011-01-26</td>\n",
       "      <td>fWKvX83p0-ka4JS3dc6E5A</td>\n",
       "      <td>5</td>\n",
       "      <td>My wife took me here on my birthday for breakf...</td>\n",
       "      <td>review</td>\n",
       "      <td>rLtl8ZkDX5vH5nAx9C3q5Q</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ZRJwVLyzEJq1VAihDhYiow</td>\n",
       "      <td>2011-07-27</td>\n",
       "      <td>IjZ33sJrzXqU-0X6U8NwyA</td>\n",
       "      <td>5</td>\n",
       "      <td>I have no idea why some people give bad review...</td>\n",
       "      <td>review</td>\n",
       "      <td>0a2KyEL0d3Yb1V6aivbIuQ</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1345</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6oRAC4uyJCsJl1X0WZpVSA</td>\n",
       "      <td>2012-06-14</td>\n",
       "      <td>IESLBzqUCLdSzSqm0eCSxQ</td>\n",
       "      <td>4</td>\n",
       "      <td>love the gyro plate. Rice is so good and I als...</td>\n",
       "      <td>review</td>\n",
       "      <td>0hT2KtfLiobPvh6cDC8JQg</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>76</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>_1QQZuf4zZOyFCvXc0o6Vg</td>\n",
       "      <td>2010-05-27</td>\n",
       "      <td>G-WvGaISbqqaMHlNnByodA</td>\n",
       "      <td>5</td>\n",
       "      <td>Rosie, Dakota, and I LOVE Chaparral Dog Park!!...</td>\n",
       "      <td>review</td>\n",
       "      <td>uZetl9T0NcROGOyFfughhg</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>419</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6ozycU1RpktNG2-1BroVtw</td>\n",
       "      <td>2012-01-05</td>\n",
       "      <td>1uJFq2r5QfJG_6ExMRCaGw</td>\n",
       "      <td>5</td>\n",
       "      <td>General Manager Scott Petello is a good egg!!!...</td>\n",
       "      <td>review</td>\n",
       "      <td>vYmM4KTsC8ZfQBg-j5MWkw</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>469</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>-yxfBYGB6SEqszmxJxd97A</td>\n",
       "      <td>2007-12-13</td>\n",
       "      <td>m2CKSsepBCoRYWxiRUsxAg</td>\n",
       "      <td>4</td>\n",
       "      <td>Quiessence is, simply put, beautiful.  Full wi...</td>\n",
       "      <td>review</td>\n",
       "      <td>sqYN3lNgvPbPCTRsMFu27g</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>2094</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>zp713qNhx8d9KCJJnrw1xA</td>\n",
       "      <td>2010-02-12</td>\n",
       "      <td>riFQ3vxNpP4rWLk_CSri2A</td>\n",
       "      <td>5</td>\n",
       "      <td>Drop what you're doing and drive here. After I...</td>\n",
       "      <td>review</td>\n",
       "      <td>wFweIWhv2fREZV_dYkz_1g</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>4</td>\n",
       "      <td>1565</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>hW0Ne_HTHEAgGF1rAdmR-g</td>\n",
       "      <td>2012-07-12</td>\n",
       "      <td>JL7GXJ9u4YMx7Rzs05NfiQ</td>\n",
       "      <td>4</td>\n",
       "      <td>Luckily, I didn't have to travel far to make m...</td>\n",
       "      <td>review</td>\n",
       "      <td>1ieuYcKS7zeAv_U15AB13A</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>274</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>wNUea3IXZWD63bbOQaOH-g</td>\n",
       "      <td>2012-08-17</td>\n",
       "      <td>XtnfnYmnJYi71yIuGsXIUA</td>\n",
       "      <td>4</td>\n",
       "      <td>Definitely come for Happy hour! Prices are ama...</td>\n",
       "      <td>review</td>\n",
       "      <td>Vh_DlizgGhSqQh4qfZ2h6A</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>349</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>nMHhuYan8e3cONo3PornJA</td>\n",
       "      <td>2010-08-11</td>\n",
       "      <td>jJAIXA46pU1swYyRCdfXtQ</td>\n",
       "      <td>5</td>\n",
       "      <td>Nobuo shows his unique talents with everything...</td>\n",
       "      <td>review</td>\n",
       "      <td>sUNkXg8-KFtCMQDV6zRzQg</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>186</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              business_id        date               review_id  stars  \\\n",
       "0  9yKzy9PApeiPPOUJEtnvkg  2011-01-26  fWKvX83p0-ka4JS3dc6E5A      5   \n",
       "1  ZRJwVLyzEJq1VAihDhYiow  2011-07-27  IjZ33sJrzXqU-0X6U8NwyA      5   \n",
       "2  6oRAC4uyJCsJl1X0WZpVSA  2012-06-14  IESLBzqUCLdSzSqm0eCSxQ      4   \n",
       "3  _1QQZuf4zZOyFCvXc0o6Vg  2010-05-27  G-WvGaISbqqaMHlNnByodA      5   \n",
       "4  6ozycU1RpktNG2-1BroVtw  2012-01-05  1uJFq2r5QfJG_6ExMRCaGw      5   \n",
       "5  -yxfBYGB6SEqszmxJxd97A  2007-12-13  m2CKSsepBCoRYWxiRUsxAg      4   \n",
       "6  zp713qNhx8d9KCJJnrw1xA  2010-02-12  riFQ3vxNpP4rWLk_CSri2A      5   \n",
       "7  hW0Ne_HTHEAgGF1rAdmR-g  2012-07-12  JL7GXJ9u4YMx7Rzs05NfiQ      4   \n",
       "8  wNUea3IXZWD63bbOQaOH-g  2012-08-17  XtnfnYmnJYi71yIuGsXIUA      4   \n",
       "9  nMHhuYan8e3cONo3PornJA  2010-08-11  jJAIXA46pU1swYyRCdfXtQ      5   \n",
       "\n",
       "                                                text    type  \\\n",
       "0  My wife took me here on my birthday for breakf...  review   \n",
       "1  I have no idea why some people give bad review...  review   \n",
       "2  love the gyro plate. Rice is so good and I als...  review   \n",
       "3  Rosie, Dakota, and I LOVE Chaparral Dog Park!!...  review   \n",
       "4  General Manager Scott Petello is a good egg!!!...  review   \n",
       "5  Quiessence is, simply put, beautiful.  Full wi...  review   \n",
       "6  Drop what you're doing and drive here. After I...  review   \n",
       "7  Luckily, I didn't have to travel far to make m...  review   \n",
       "8  Definitely come for Happy hour! Prices are ama...  review   \n",
       "9  Nobuo shows his unique talents with everything...  review   \n",
       "\n",
       "                  user_id  cool  useful  funny  length  \n",
       "0  rLtl8ZkDX5vH5nAx9C3q5Q     2       5      0     889  \n",
       "1  0a2KyEL0d3Yb1V6aivbIuQ     0       0      0    1345  \n",
       "2  0hT2KtfLiobPvh6cDC8JQg     0       1      0      76  \n",
       "3  uZetl9T0NcROGOyFfughhg     1       2      0     419  \n",
       "4  vYmM4KTsC8ZfQBg-j5MWkw     0       0      0     469  \n",
       "5  sqYN3lNgvPbPCTRsMFu27g     4       3      1    2094  \n",
       "6  wFweIWhv2fREZV_dYkz_1g     7       7      4    1565  \n",
       "7  1ieuYcKS7zeAv_U15AB13A     0       1      0     274  \n",
       "8  Vh_DlizgGhSqQh4qfZ2h6A     0       0      0     349  \n",
       "9  sUNkXg8-KFtCMQDV6zRzQg     0       1      0     186  "
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yelp.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#define a function that accepts text and returns polarity\n",
    "def detect_sentiment(text):\n",
    "    return TextBlob(text.decode('utf-8')).sentiment.polarity #Python 2\n",
    "    #return TextBlob(text).sentiment.polarity Python 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a new DataFrame column for sentiment\n",
    "yelp['sentiment'] = yelp.text.apply(detect_sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x3af4f630>"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAhsAAAGICAYAAADoPSLNAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x3d700eb8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "yelp.boxplot(column='sentiment', by='stars')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
